\subsubsection{Setting}

We consider supervised regression/classification with labeled data $(\vx, \vy)
\in \sX \times \sY$ generated by a distribution $\pdata(\vx, \vy)$. The training
set $\sD = \{ (\vx_n, \vy_n)\}_{n=1}^N$ consists of $N$ \iid samples from
$\pdata$ and the deep model $f_{\vtheta}: \sX \rightarrow \sF$ maps inputs
$\vx_n$ to predictions $f_{\vtheta}(\vx_n)$ by parameters $\vtheta \in \sTheta
:= \sR^D$. This prediction is evaluated by a loss function $\ell : \sF \times
\sY \rightarrow \R$ which compares to the label $\vy_n$. The goal is minimizing
an inaccessible expected risk $\gL_{\pdata}(\vtheta) = \int
\ell(f_{\vtheta}(\vx), \vy) \ \rd \pdata(\vx, \vy)$ by empirical approximation
through $\gL_{\sD}(\vtheta) = \nicefrac{1}{N} \sum_{n=1}^N
\ell(f_{\vtheta}(\vx_n), \vy_n) := \nicefrac{1}{N} \sum_{n=1}^N
\ell_n(\vtheta)$, which in practice is stochastically sub-sampled on
mini-batches $\sB \subset \sD$,
\begin{align}
  \label{cockpit::eq:mini-batch-loss}
  \gL_{\sB}(\vtheta) = \frac{1}{|\sB|} \sum_{(\vx_n,\vy_n) \in\sB} \ell_n(\vtheta)\,.
\end{align}
As is standard practice, we use first- and second-order information of the
mini-batch loss, described by its gradient $\vg_{\sB}(\vtheta)$ and Hessian
$\mH_{\sB}(\vtheta)$,
\begin{align}
  \vg_{\sB}(\vtheta)
  =
  \frac{1}{|\sB|} \sum_{(\vx_n, \vy_n) \in \sB}
  \grad{\vtheta}\ell_n(\vtheta)\, ,
  \quad
  \mH_{\sB}(\vtheta)
  =
  \frac{1}{|\sB|} \sum_{(\vx_n, \vy_n) \in \sB}
  \gradsquared{\vtheta} \ell_n(\vtheta)\,.
\end{align}

\subsubsection{Design Choices}

To minimize computational and design overhead, we restrict the metrics to
quantities that require no additional model evaluations. This means that, at
training step $t \to t + 1$ with mini-batches $\sB_t, \sB_{t+1}$ and parameters
$\vtheta_t, \vtheta_{t+1}$, we access information about the mini-batch losses
$\gL_{\sB_t}(\vtheta_t)$ and $\gL_{\sB_{t+1}}(\vtheta_{t+1})$, but no
cross-terms that require additional forward passes.

\subsubsection{Key Point}

$\gL_{\sB}(\vtheta), \vg_{\sB}(\vtheta)$, and $\mH_{\sB}(\vtheta)$ are just
expected values of a \textit{distribution} over the batch. Only recently, this
distribution has begun to attract attention \citep{faghri2020study} as its
computation has become more accessible \citep{bradbury2018jax,
  dangel2020backpack}. Contemporary optimizers leverage only the \emph{mean}
gradient and neglect higher moments. One core point of our work is making
extensive use of these distribution properties, trying to visualize them in
various ways. This distinguishes \cockpit from being ``just a collection of
plots'' that could be built in tools like \tensorboard. Leveraging these
distributional quantities, we create instruments and show how they can help
adapt hyperparameters (\Cref{cockpit::sec:adapting_hyperparameters}), analyze
the loss landscape (\Cref{cockpit::sec:curvature}), and track network dynamics
(\Cref{cockpit::sec:network_dynamics}). Instruments can sometimes be built from
already-computed information or are efficient variants of previously proposed
observables. To keep the presentation concise, we highlight the instruments
shown in \Cref{cockpit::fig:showcase} and listed in
\Cref{cockpit::tab:overview-quantities}. \Cref{cockpit::app:instruments} defines
them formally and contains more extensions, such as the mean \gsnr
\citep{liu2020understanding}, the early stopping \citep{mahsereci2017early} and
\cabs \citep{balles2017coupling} criterion, which can all be used in \cockpit.

\input{figures/cockpit/overview_quantities}

\subsection{Adapting Hyperparameters}\label{cockpit::sec:adapting_hyperparameters}
One big challenge in deep learning is setting the hyperparameters correctly,
which is currently mostly done by trial \& error through parameter searches. We
aim to augment this process with instruments that inform the user about the
effect that the chosen parameters have on the current training process.

\subsubsection{\robustInlinecode{Alpha}: Are We Crossing the Valley?}

Using individual loss and gradient observations at the start and end point of
each iteration, we build a noise-informed uni-variate quadratic approximation
along the step direction (\ie the loss as a function of the step size), and
assess to which point on this parabola our optimizer moves. We standardize this
value $\alpha$ such that stepping to the valley-floor is assigned $\alpha=0$,
the starting point is at $\alpha=-1$ and updates to the point exactly opposite
of the starting point have $\alpha=1$ (see \Cref{cockpit::app:alpha} for a more
detailed visual and mathematical description of $\alpha$).
\Cref{cockpit::fig:LINE} illustrates the scenarios $\alpha=\pm1$ and how
monitoring the $\alpha$-distribution (right panel) can help distinguish between
two training runs with similar performance but distinct failure sources. By
default, this \cockpit instrument shows the $\alpha$-distribution for the last
10\,\% of training and the entire training process (top left plot in
\Cref{cockpit::fig:showcase}). In \Cref{cockpit::sec:alpha_exp} we show
empirically that, \emph{counter-intuitively}, it is generally \emph{not} a good
idea to choose the step size such that $\alpha$ is close to zero.

\subsubsection{Distances: Are We Making Progress?}

Another way to discern the trajectories in \Cref{cockpit::fig:LINE} is by
measuring the $L_2$ \textit{distance from initialization}
\citep{nagarajan2019generalization} and the \textit{update size}
\citep{agrawal2020investigating,frankle2020early} in parameter space. Both are
shown together in one \cockpit instrument (see also center-left plot in
\Cref{cockpit::fig:showcase}) and are far larger for the blue line in
\Cref{cockpit::fig:LINE}. These distance metrics are also able to disentangle
phases for the blue path. Using the same step size, it will continue to ``jump
back and forth'' between the loss valley's walls but at some point cease to make
progress. During this ``surfing of the walls'', the \textit{distance from
  initialization} increases, ultimately though, it will stagnate, with the
\textit{update size} remaining non-zero, indicating diffusion. While the initial
``surfing the wall''-phase benefits training (see
\Cref{cockpit::sec:alpha_exp}), achieving stationarity may require adaptation
once the optimizer reaches that diffusion.

\subsubsection{Gradient Norm: How Steep Is the Wall?}

The \textit{update size} will show that the orange trajectory is stuck. But why?
Such slow-down can result from both a bad learning rate and from loss landscape
plateaus. The \textit{gradient norm} (bottom left panel in
\Cref{cockpit::fig:showcase}) distinguishes these two causes.

\subsubsection{Gradient Tests: How Noisy Is the Batch?}

The batch size trades off gradient accuracy versus computational cost. Recently,
adaptive sampling strategies based on testing geometric constraints between mean
and individual gradients have been proposed
\citep{byrd2012sample,bollapragada2017adaptive}. The \textit{norm},
\textit{inner product}, and \textit{orthogonality tests} use a standardized
radius and two band widths (parallel and orthogonal to the gradient mean) that
indicate how strongly individual gradients scatter around the mean. The original
works use these values to adapt batch sizes. Instead, \cockpit combines all
three tests into a single gauge (top middle plot of
\Cref{cockpit::fig:showcase}) using the standardized noise radius and band
widths for visualization. These noise signals can be used to guide batch size
adaptation on- and offline, or to probe the influence of gradient alignment on
training speed \citep{sankararaman2020impact} and generalization
\citep{chatterjee2020coherent,chatterjee2020making,liu2020understanding}.

\subsection{Hessian Properties for Local Loss Geometry}\label{cockpit::sec:curvature}
An intuition for the local loss landscape helps in many ways. It can help
diagnose whether training is stuck, to adapt the step size, and explain
stability or regularization \citep{ginsburg2020regularization,jastrzebski2020break}. The key
challenge is the large number of weights: low-dimensional projections of
surfaces can behave unintuitively \citep{mulayoff2020unique}, but tracking the extreme
or average behaviors may help in debugging, especially if first-order metrics
fail.

\subsubsection{Hessian Eigenvalues: A Gorge or a Lake?}

In convex optimization, the maximum Hessian eigenvalue crucially determines the
appropriate step size \citep{schmidt2014convergence}. Many works have studied
the Hessian spectrum in machine learning
\citep[\eg][]{ghorbani2019investigation,ginsburg2020regularization,mulayoff2020unique,sagun2017eigenvalues,sagun2018empirical,yao2020pyhessian}.
In short: curvature matters. Established \citep{pearlmutter1994fast} and recent
autodiff frameworks \citep{dangel2020backpack} can compute Hessian properties
without requiring the full matrix. \cockpit leverages this to provide the
\textit{Hessian's largest eigenvalue} and \textit{trace} (right top and middle
plots in \Cref{cockpit::fig:showcase}). The former resembles the loss surface's
sharpest valley and can thus hint at training instabilities
\citep{jastrzebski2020break}. The \textit{trace} describes a notion of ``average
curvature'', since the eigenvalues $\lambda_i$ relate to it by $\sum_i \lambda_i
= \Tr(\mH_{\sB}(\vtheta))$, which might correlate with generalization
\citep{jastrzebski2021catastrophic}.

\subsubsection{TIC: How Do Curvature \& Gradient Noise Interact?}

There is an ongoing debate about curvature's link to generalization
\citep[\eg][]{dinh2017sharp,hochreiter1997flat,keskar2017large}. The
\emph{Takeuchi Information Criterion (TIC)}
\citep{takeuchi1976distribution,thomas2020interplay} estimates the
generalization gap by a ratio between Hessian and non-central second gradient
moment. It also provides intuition for changes in the objective function implied
by gradient noise. Inspired by \cite{thomas2020interplay}, \cockpit provides mini-batch TIC estimates (bottom
right plot of \Cref{cockpit::fig:showcase}).

\subsection{Visualizing Internal Network Dynamics}\label{cockpit::sec:network_dynamics}
Histograms are a natural visual compression of the high-dimensional $|\sB|
\times D$ individual gradient values. They give insights into the gradient
\emph{distribution} and hence offer a more detailed view of the learning signal.
Together with the parameter associated to each individual gradient, the entire
model status and dynamics can be visualized in a single plot and be monitored
during training. This provides a more fine-grained view of training compared to
tracking parameters and gradient norms \citep{frankle2020early}.

\subsubsection{Gradient \& Parameter Histograms: What Is Happening in Our Net?}

\cockpit offers a uni-variate \textit{histogram of the gradient elements}, \ie
the numbers
$\{[\vg_n( \vtheta )]_j\}_{(\vx_{n}, \vy_{n})\in \sB}^{j=1,\dots,D}$.
Additionally, a combined \textit{histogram of parameter-gradient pairs}
$\{([\vtheta]_j, [\vg_n( \vtheta )]_j\}_{(\vx_{n}, \vy_{n})\in \sB}^{j=1,\dots,D}$
provides a two-dimensional look into the network's gradient and parameter values
in a mini-batch. \Cref{cockpit::sec:misscaled_data_exp} shows an example
use-case of the gradient histogram; \Cref{cockpit::sec:vanishing_gradient_exp}
makes the case for the layer-wise variants of the instruments.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../thesis"
%%% End:
